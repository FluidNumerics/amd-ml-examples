{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# ðŸ¤— Hugging Face transformers\n",
    "# https://huggingface.co/docs/transformers/index\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the correct device is being used\n",
    "# E.g. 'AMD Radeon Pro w7800'\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# set device to 'cuda' for ROCm GPUs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No CUDA device found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# verify the device is set to 'cuda'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to local model\n",
    "path_to_model = (\n",
    "    f\"{os.getcwd()}/fine-tuning-llama-3/Llama-Math-Single-GPU/checkpoint-8000\"\n",
    ")\n",
    "print(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "\n",
    "fp4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4bit quantization\n",
    "    bnb_4bit_quant_type=\"fp4\",  # Use FP4 datatype (\"nf4\" alternative)\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Computational type might be different than input type\n",
    "    bnb_4bit_quant_storage=torch.float16,\n",
    ")\n",
    "\n",
    "my_model = LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    quantization_config=fp4_config,\n",
    ")\n",
    "\n",
    "adapted_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=my_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = r\"Johnny has three apples. Jane has fourteen oranges. Jane says that she will trade three oranges for one apple. What is the maximum number of oranges that Johnny could trade for?\"\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=10,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"sequence: {i}\")\n",
    "    print(seq[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
