{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Fluid Numerics LLC\n",
    "#          Garrett Byrd             (garrett@fluidnumerics.com)\n",
    "#          Dr. Joseph Schoonover    (joe@fluidnumerics.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the correct device is being used\n",
    "# E.g. 'AMD Radeon Pro w7800'\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to local model\n",
    "path_to_model = \"/home/garrett/amd/misc/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# If not using a local model, this can be set as the name of a model on hugging face, e.g.\n",
    "# path_to_model = \"meta-llama/llama-3-8b-instruct\"\n",
    "# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "# set device to 'cuda' for ROCm GPUs, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# verify the device is set to 'cuda'\n",
    "print(f\"--------\\nDevice: {device}\\n--------\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
