{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama-3 on AMD Radeon GPU\n",
    "\n",
    "## Authors:\n",
    "### **Fluid Numerics**\n",
    "* **Garrett Byrd**             ([garrett@fluidnumerics.com](garrett@fluidnumerics.com))\n",
    "* **Dr. Joseph Schoonover**    ([joe@fluidnumerics.com](joe@fluidnumerics.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "### Software\n",
    "* ROCm 6.1\n",
    "* Python 3.12\n",
    "\n",
    "### Hardware\n",
    "* A `gfx1100` Radeon (or Radeon Pro) GPU ([check here](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Package List\n",
    "\n",
    "We recommend using Anaconda/Miniconda as your Python environment manager. Install Miniconda [here](https://docs.anaconda.com/miniconda/).\n",
    "\n",
    "1. PyTorch+rocm6.1 (Install ROCm-compatible PyTorch [here](2.4.0+rocm6.1)) (`pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.1`)\n",
    "\n",
    "2. Install Hugging Face Libraries (`pip install transformers datasets accelerate peft evaluate trl`)\n",
    "\n",
    "3. If you are running this notebook, be sure to install Jupyter (`pip install jupyterlab`)\n",
    "\n",
    "\n",
    "## Installing `bitsandbytes`\n",
    "Install the **rocm_enabled** branch of the **ROCm/bitsandbytes** repository [found here](https://github.com/ROCm/bitsandbytes/tree/rocm_enabled). \n",
    "\n",
    "Note: this is the branch actively developed by the ROCm team and is different than the branches on the [main bitsandbytes repository](https://github.com/bitsandbytes-foundation/bitsandbytes). At the time of writing, neither of the main repository branches `main` or `multi-backend-refactor` will work for this example.\n",
    "\n",
    "To install:\n",
    "```sh\n",
    "git clone --recurse https://github.com/ROCm/bitsandbytes\n",
    "cd bitsandbytes\n",
    "git checkout rocm_enabled\n",
    "pip install -r requirements-dev.txt\n",
    "cmake -DCOMPUTE_BACKEND=hip -S . -DBNB_ROCM_ARCH=\"gfx1100\"\n",
    "make\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from numpy import argmax\n",
    "\n",
    "# ðŸ¤— Hugging Face Libraries\n",
    "\n",
    "# transformers\n",
    "# https://huggingface.co/docs/transformers/index\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# datasets\n",
    "# https://huggingface.co/docs/datasets/index\n",
    "from datasets import load_dataset\n",
    "\n",
    "# peft\n",
    "# https://huggingface.co/docs/peft/index\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# evaluate\n",
    "# https://huggingface.co/docs/evaluate/index\n",
    "import evaluate\n",
    "\n",
    "# trl (Transformer Reinforcement Learning)\n",
    "# https://huggingface.co/docs/trl/en/index\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rocm-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the correct device is being used\n",
    "# E.g. 'AMD Radeon Pro w7800'\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"Device name: {torch.cuda.get_device_name(1)}\")\n",
    "\n",
    "# set device to 'cuda' for ROCm GPUs, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# verify the device is set to 'cuda'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to local model\n",
    "path_to_model = \"/home/garrett/amd/misc/Meta-Llama-3-8B\"\n",
    "\n",
    "# If not using a local model, this can be set as the name of a model on hugging face, e.g.\n",
    "# path_to_model = \"meta-llama/llama-3-8b\"\n",
    "# https://huggingface.co/meta-llama/Meta-Llama-3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/output before training\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(path_to_model)  # Load model tokenizer\n",
    "my_tokenizer.pad_token = my_tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "# BitsandBytes config\n",
    "fp4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4bit quantization\n",
    "    bnb_4bit_quant_type=\"fp4\",  # Use FP4 datatype (\"nf4\" alternative)\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Computational type might be different than input type\n",
    ")\n",
    "\n",
    "quantized_model = LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,  # Set model\n",
    "    quantization_config=fp4_config,  # Apply config\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "quantized_model_architecture = quantized_model.__str__()\n",
    "\n",
    "# Sample prompt (evaluates to 1/4 - 3/(4e**2))\n",
    "prompt = r\"Evaluate the integral $\\int_0^1 x e^{-2x} dx$.\"\n",
    "prompt = r\"Johnny has three apples. Jane has fourteen oranges. Jane says that she will trade three oranges for one apple. What is the maximum number of oranges that Johnny could trade for?\"\n",
    "\n",
    "quantized_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=quantized_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = quantized_pipeline(\n",
    "    text_inputs=prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    temperature=2.0,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA (Low-Rank Adaptation)\n",
    "# https://huggingface.co/docs/peft/main/en/developer_guides/lora\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Size of adapter layer\n",
    "    lora_alpha=16,  # \"How strongly does the adaptation layer affect the base model?\" (see 4.1 of https://arxiv.org/abs/2106.09685)\n",
    "    lora_dropout=0.05,  # Optional dropout layer\n",
    "    bias=\"none\",  # No bias\n",
    "    task_type=\"CAUSAL_LM\",  # Task type, see https://huggingface.co/docs/peft/en/package_reference/peft_types#peft.TaskType\n",
    "    target_modules=[  # Which modules to apply adapter layers to?\n",
    "        \"up_proj\",  # up projection\n",
    "        \"down_proj\",  # down projection\n",
    "        \"gate_proj\",  # gate projection\n",
    "        \"k_proj\",  # Key\n",
    "        \"q_proj\",  # Query\n",
    "        \"v_proj\",  # Value\n",
    "        \"o_proj\",  # Output\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply the LoRA config\n",
    "adapted_model = get_peft_model(quantized_model, lora_config)\n",
    "\n",
    "# output should be less sensible\n",
    "prompt = r\"Evaluate the integral $\\int_0^1 x e^{-2x} dx$.\"\n",
    "\n",
    "adapted_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=adapted_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# https://huggingface.co/datasets/meta-math/MetaMathQA\n",
    "MetaMathQA = load_dataset(\n",
    "    \"json\", data_files=\"MetaMathQA/MetaMathQA-395K.json\", split=\"train[:10000]\"\n",
    ")\n",
    "# Split dataset into \"test\" and \"train\" columns\n",
    "MetaMathQA = MetaMathQA.train_test_split(test_size=0.2)\n",
    "\n",
    "print(MetaMathQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat templates are Jinja template strings\n",
    "# https://huggingface.co/blog/chat-templates\n",
    "\n",
    "\n",
    "# Format\n",
    "def instructify(qr_row):\n",
    "    qr_json = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": qr_row[\"query\"],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": qr_row[\"response\"],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    qr_row[\"text\"] = my_tokenizer.apply_chat_template(qr_json, tokenize=False)\n",
    "    return qr_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "my_tokenizer.chat_template = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
    "\n",
    "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "' }}{% endif %}\"\"\"\n",
    "\n",
    "my_tokenizer.chat_template = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message['content'] | trim + '\\n' %}{{ content }}{% endfor %}\"\"\"\n",
    "\n",
    "print(my_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = MetaMathQA.map(instructify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset[\"train\"][0][\"text\"])\n",
    "print(formatted_dataset[\"test\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\"query\": \\n')\n",
    "print(formatted_dataset[\"test\"][0][\"query\"], \"\\n\")\n",
    "print('\"response\": \\n')\n",
    "print(formatted_dataset[\"test\"][0][\"response\"], \"\\n\")\n",
    "print('\"text\": \\n')\n",
    "print(formatted_dataset[\"test\"][0][\"text\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = formatted_dataset[\"test\"][0][\"query\"]\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=example_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/evaluate/package_reference/loading_methods#evaluate.load\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = argmax(logits, axis=-1)\n",
    "    return evaluate.metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "# https://huggingface.co/docs/transformers/v4.45.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "# https://huggingface.co/docs/transformers/en/perf_train_gpu_one\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"Llama-Math-TEMP\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",  # complete list: https://github.com/huggingface/transformers/blob/a43e84cb3b78fcac3d5d9374a8488f74f3f19245/src/transformers/training_args.py#L144\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.25,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Fine-Tuning Trainer\n",
    "# https://huggingface.co/docs/trl/en/sft_trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=adapted_model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=my_tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rocm-smi\n",
    "# memory: 13.76GB\n",
    "# training time: 101m 40.6s\n",
    "# average temperature: ~76C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
