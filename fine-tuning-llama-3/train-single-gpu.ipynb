{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama-3 on AMD Radeon GPU\n",
    "\n",
    "## Authors:\n",
    "### **Fluid Numerics**\n",
    "* **Garrett Byrd**             ([garrett@fluidnumerics.com](garrett@fluidnumerics.com))\n",
    "* **Dr. Joseph Schoonover**    ([joe@fluidnumerics.com](joe@fluidnumerics.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "### Software\n",
    "* ROCm 6.1\n",
    "* Python 3.12\n",
    "\n",
    "### Hardware\n",
    "* A `gfx1100` Radeon (or Radeon Pro) GPU ([check here](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Package List\n",
    "\n",
    "We recommend using Anaconda/Miniconda as your Python environment manager. Install Miniconda [here](https://docs.anaconda.com/miniconda/).\n",
    "\n",
    "1. PyTorch+rocm6.1 (Install ROCm-compatible PyTorch [here](2.4.0+rocm6.1)) (`pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.1`)\n",
    "\n",
    "2. Install Hugging Face Libraries (`pip install transformers datasets accelerate peft evaluate trl`)\n",
    "\n",
    "3. If you are running this notebook, be sure to install Jupyter (`pip install jupyterlab`)\n",
    "\n",
    "\n",
    "## Installing `bitsandbytes`\n",
    "Install the **rocm_enabled** branch of the **ROCm/bitsandbytes** repository [found here](https://github.com/ROCm/bitsandbytes/tree/rocm_enabled). \n",
    "\n",
    "Note: this is the branch actively developed by the ROCm team and is different than the branches on the [main bitsandbytes repository](https://github.com/bitsandbytes-foundation/bitsandbytes). At the time of writing, neither of the main repository branches `main` or `multi-backend-refactor` are tested for this example.\n",
    "\n",
    "To install:\n",
    "```sh\n",
    "git clone --recurse https://github.com/ROCm/bitsandbytes\n",
    "cd bitsandbytes\n",
    "git checkout rocm_enabled\n",
    "pip install -r requirements-dev.txt\n",
    "cmake -DCOMPUTE_BACKEND=hip -S . -DBNB_ROCM_ARCH=\"gfx1100\"\n",
    "make\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from numpy import argmax\n",
    "\n",
    "# ðŸ¤— Hugging Face Libraries\n",
    "\n",
    "# transformers\n",
    "# https://huggingface.co/docs/transformers/index\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# datasets\n",
    "# https://huggingface.co/docs/datasets/index\n",
    "from datasets import load_dataset\n",
    "\n",
    "# peft\n",
    "# https://huggingface.co/docs/peft/index\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# evaluate\n",
    "# https://huggingface.co/docs/evaluate/index\n",
    "import evaluate\n",
    "\n",
    "# trl (Transformer Reinforcement Learning)\n",
    "# https://huggingface.co/docs/trl/en/index\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the correct device is being used\n",
    "# E.g. 'AMD Radeon Pro w7800'\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# set device to 'cuda' for ROCm GPUs, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# verify the device is set to 'cuda'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to local model\n",
    "path_to_model = f\"{os.getcwd()}/fine-tuning-llama-3/Meta-Llama-3-8B\"\n",
    "\n",
    "# If not using a local model, this can be set as the name of a model on hugging face, e.g.\n",
    "# path_to_model = \"meta-llama/llama-3-8b\"\n",
    "# https://huggingface.co/meta-llama/Meta-Llama-3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/output before training\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(path_to_model)  # Load model tokenizer\n",
    "my_tokenizer.pad_token = my_tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "# BitsandBytes config\n",
    "fp4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4bit quantization\n",
    "    bnb_4bit_quant_type=\"fp4\",  # Use FP4 datatype (\"nf4\" alternative)\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Computational type might be different than input type\n",
    ")\n",
    "\n",
    "quantized_model = LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,  # Set model\n",
    "    quantization_config=fp4_config,  # Apply config\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Two sample prompts\n",
    "prompt1 = (\n",
    "    r\"Evaluate the integral $\\int_0^1 x e^{-2x} dx$.\"  # (evaluates to 1/4 - 3/(4e**2))\n",
    ")\n",
    "prompt2 = r\"Johnny has three apples. Jane has fourteen oranges. Jane says that she will trade three oranges for one apple. What is the maximum number of oranges that Johnny could trade for?\"\n",
    "\n",
    "quantized_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=quantized_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = quantized_pipeline(\n",
    "    text_inputs=prompt2,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    temperature=2.0,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA (Low-Rank Adaptation)\n",
    "# https://huggingface.co/docs/peft/main/en/developer_guides/lora\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Size of adapter layer\n",
    "    lora_alpha=16,  # \"How strongly does the adaptation layer affect the base model?\" (see 4.1 of https://arxiv.org/abs/2106.09685)\n",
    "    lora_dropout=0.05,  # Optional dropout layer\n",
    "    bias=\"none\",  # No bias\n",
    "    task_type=\"CAUSAL_LM\",  # Task type, see https://huggingface.co/docs/peft/en/package_reference/peft_types#peft.TaskType\n",
    "    target_modules=[  # Which modules to apply adapter layers to?\n",
    "        \"up_proj\",  # up projection\n",
    "        \"down_proj\",  # down projection\n",
    "        \"gate_proj\",  # gate projection\n",
    "        \"k_proj\",  # Key\n",
    "        \"q_proj\",  # Query\n",
    "        \"v_proj\",  # Value\n",
    "        \"o_proj\",  # Output\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply the LoRA config\n",
    "adapted_model = get_peft_model(quantized_model, lora_config)\n",
    "\n",
    "# output should be less sensible\n",
    "\n",
    "adapted_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=adapted_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=prompt2,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# https://huggingface.co/datasets/meta-math/MetaMathQA\n",
    "# load first 10 000 rows\n",
    "MetaMathQA = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"fine-tuning-llama-3/MetaMathQA/MetaMathQA-395K.json\",\n",
    "    split=\"train[:10000]\",\n",
    ")\n",
    "# Split dataset into \"train\" and \"test\" columns (8000/2000 split)\n",
    "MetaMathQA = MetaMathQA.train_test_split(test_size=0.2)\n",
    "\n",
    "print(MetaMathQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat templates are Jinja template strings\n",
    "# https://huggingface.co/blog/chat-templates\n",
    "\n",
    "\n",
    "# Format\n",
    "def instructify(qr_row):\n",
    "    qr_json = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": qr_row[\"query\"],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": qr_row[\"response\"],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    qr_row[\"text\"] = my_tokenizer.apply_chat_template(qr_json, tokenize=False)\n",
    "    return qr_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "# Llama-3-Instruct uses the following chat template:\n",
    "# my_tokenizer.chat_template = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
    "\n",
    "# '+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# ' }}{% endif %}\"\"\"\n",
    "\n",
    "# But concatenating query/response is sufficient for our uses\n",
    "my_tokenizer.chat_template = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message['content'] | trim + '\\n' %}{{ content }}{% endfor %}\"\"\"\n",
    "\n",
    "print(my_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the chat template to our data\n",
    "formatted_dataset = MetaMathQA.map(instructify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \"text\" columns\n",
    "print(formatted_dataset[\"train\"][0][\"text\"])\n",
    "print(formatted_dataset[\"test\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = formatted_dataset[\"test\"][0][\"query\"]\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=example_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/evaluate/package_reference/loading_methods#evaluate.load\n",
    "# https://huggingface.co/docs/evaluate/v0.4.0/en/types_of_evaluations#metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = argmax(logits, axis=-1)\n",
    "    return evaluate.metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "# https://huggingface.co/docs/transformers/v4.45.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "# https://huggingface.co/docs/transformers/en/perf_train_gpu_one\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"Llama-Math-Single-GPU-noether\",  # Output directory\n",
    "    per_device_train_batch_size=8,  # Training batch size\n",
    "    per_device_eval_batch_size=8,  # Evaluation batch size\n",
    "    gradient_accumulation_steps=1,  # Gradient accumulation steps\n",
    "    optim=\"paged_adamw_8bit\",  # Which optimizer; complete list: https://github.com/huggingface/transformers/blob/a43e84cb3b78fcac3d5d9374a8488f74f3f19245/src/transformers/training_args.py#L144\n",
    "    num_train_epochs=1,  # How many epochs\n",
    "    eval_strategy=\"steps\",  # The evaluation strategy to adopt during training.\n",
    "    eval_steps=0.5,  # When to evaluate in each epoch\n",
    "    logging_steps=1,  # Number of update steps between two logs if logging_strategy=\"steps\"\n",
    "    warmup_steps=10,  # Number of steps used for a linear warmup from 0 to learning_rate\n",
    "    logging_strategy=\"steps\",  # The logging strategy to adopt during training.\n",
    "    learning_rate=1e-4,  # Learning rate\n",
    "    fp16=True,  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    bf16=False,  # Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    group_by_length=True,  # Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Fine-Tuning Trainer\n",
    "# https://huggingface.co/docs/trl/en/sft_trainer\n",
    "# Consider updating to use SFTConfig https://huggingface.co/docs/trl/en/sft_trainer#trl.SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model=adapted_model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=my_tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
