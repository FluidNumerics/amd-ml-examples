{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Fluid Numerics LLC\n",
    "#          Garrett Byrd             (garrett@fluidnumerics.com)\n",
    "#          Dr. Joseph Schoonover    (joe@fluidnumerics.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the correct device is being used\n",
    "# E.g. 'AMD Instinct MI210'\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to local model\n",
    "path_to_model = \"/scratch/joe/llama-2-7b-chat-hf\"\n",
    "\n",
    "# set device to 'cuda' for ROCm GPUs, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# verify the device is set to 'cuda'\n",
    "print(f\"--------\\nDevice: {device}\\n--------\\n\")\n",
    "\n",
    "# AutoTokenizer is a generic tokenizer class that will be instantiated as one of the tokenizer classes \n",
    "# of the library when created with the AutoTokenizer.from_pretrained(pretrained_model_name_or_path) class method.\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoTokenizer\n",
    "\n",
    "# Instantiate one of the tokenizer classes of the library from a pre-trained model vocabulary.\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(path_to_model)\n",
    "\n",
    "# A pipeline tokenizes the input, feeds it to the model, and generates an output.\n",
    "# In this specific case, both the input and outputs are English text.\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",              \n",
    "    model=path_to_model,            \n",
    "    torch_dtype=torch.float16,      \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load the vocabulary of the model into a list\n",
    "vocab = list(tokenizer.get_vocab())\n",
    "\n",
    "# Sample the vocabulary\n",
    "sample_tokens = random.sample(vocab, 10)\n",
    "\n",
    "# Print relevant information\n",
    "print(sample_tokens)\n",
    "print(\"-----------\")\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(len(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
