{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Fluid Numerics LLC\n",
    "#          Garrett Byrd             (garrett@fluidnumerics.com)\n",
    "#          Dr. Joseph Schoonover    (joe@fluidnumerics.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the correct device is being used\n",
    "# E.g. 'AMD Instinct MI210'\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to local model\n",
    "path_to_model = \"/home/garrett/amd/misc/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# If not using a local model, this can be set as the name of a model on hugging face, e.g.\n",
    "# path_to_model = \"meta-llama/llama-3-8b-instruct\"\n",
    "# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "# set device to 'cuda' for ROCm GPUs, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# verify the device is set to 'cuda'\n",
    "print(f\"--------\\nDevice: {device}\\n--------\\n\")\n",
    "\n",
    "# model = transformers.AutoModel.from_pretrained(path_to_model)\n",
    "\n",
    "# AutoTokenizer is a generic tokenizer class that will be instantiated as one of the tokenizer classes \n",
    "# of the library when created with the AutoTokenizer.from_pretrained(pretrained_model_name_or_path) class method.\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoTokenizer\n",
    "\n",
    "# Instantiate one of the tokenizer classes of the library from a pre-trained model vocabulary.\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(path_to_model)\n",
    "\n",
    "# A pipeline tokenizes the input, feeds it to the model, and generates output.\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/pipelines\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",          # What type of model are we running?\n",
    "    model=path_to_model,        # path to local model\n",
    "    torch_dtype=torch.float16,  # set precision of tensors used by the model\n",
    "    device_map=\"auto\",          # uses the 'accelerate' package\n",
    "    \n",
    ")\n",
    "\n",
    "# Provide an input and generate a response\n",
    "prompt = 'I like listening to Snarky Puppy and Frank Zappa. What are some other musicians I might like?\\n'\n",
    "\n",
    "sequences = pipeline(\n",
    "    text_inputs=prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print()\n",
    "# For more about big models\n",
    "# https://huggingface.co/docs/accelerate/en/usage_guides/big_modeling\n",
    "\n",
    "# text_inputs:\n",
    "#       the input text\n",
    "#\n",
    "# do_sample:\n",
    "#       if False:\n",
    "#           Use \"greedy selection\";\n",
    "#           do not sample a token probabilistically,\n",
    "#           always use the token most likely to come next.\n",
    "#       if True:\n",
    "#           Sample a token probabilistically.\n",
    "#\n",
    "# top_k:\n",
    "#       When generating a token,\n",
    "#       only sample the tokens with the 'top_k' highest probabilities.\n",
    "#\n",
    "# num_return_sequences:\n",
    "#       How many token sequences (responses) to generate.\n",
    "#\n",
    "# eos_token_id\n",
    "#       What is the ID of the end of sequence token?\n",
    "#       This can differ per model, so it important to specify it from the tokenizer.\n",
    "#       In Llama 2, this is \"2\".\n",
    "#\n",
    "# https://huggingface.co/docs/transformers/en/internal/generation_utils\n",
    "\n",
    "# Print the response.\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'Can you tell me about AMD?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'Can you tell me about the company Fluid Numerics?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Huge\\text{softmax}(\\textbf{x}, T) = \\frac{\\exp(x_i/T)}{\\sum_{n=1}^{N}\\exp(x_n/T)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above,\n",
    "#       x is a vector,\n",
    "#       T is the temperature.\n",
    "#       I.e., softmax turns an arbitrary vector into a probability distribution.\n",
    "\n",
    "# temperature:\n",
    "#       Affects the probabilistic behavior of token selection.\n",
    "#\n",
    "#       A temperature that tends to zero\n",
    "#       functions identically to greedy selection.\n",
    "#\n",
    "#       A temperature that tends to infinity\n",
    "#       normalizes the probability distribution into\n",
    "#       a uniform distribution.\n",
    "#\n",
    "#       Temperature is set to 1 (one) by default.\n",
    "\n",
    "# varying temperature\n",
    "for i in range(-2,3):\n",
    "    temp = 10.0**(i/2)\n",
    "    sequences = pipeline(\n",
    "        'What is two plus two?',\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=temp\n",
    "    )\n",
    "\n",
    "    print(f\"\\n-------- temperature = {temp}\")\n",
    "    for seq in sequences:\n",
    "        print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
